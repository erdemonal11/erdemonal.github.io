<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Erdem Önal</title>
    <link rel="icon" type="image/png" href="images/icon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="header">
        <h1>Erdem Önal</h1>
        <div class="intro">
            I am a first year MSc student in <a href="https://www.mines-stetienne.fr/formation/cyber-physical-social-systems-cps2/" class="program-link" target="_blank">Computer Science (CPS2: AI and IoT)</a> at Jean Monnet University and École des Mines de Saint-Étienne.<br><br>
            I am interested in how <span class="highlight-ml">machine learning</span> can be integrated into <span class="highlight-is">intelligent systems</span>.
        </div>
        <div class="social-links">
            <a href="mailto:erdemonal11@outlook.com" title="Email">
                <i class="fas fa-envelope"></i>
            </a>
            <a href="https://linkedin.com/in/erdemonal1" title="LinkedIn" target="_blank">
                <i class="fab fa-linkedin"></i>
            </a>
            <a href="https://github.com/erdemonal11" title="GitHub" target="_blank">
                <i class="fab fa-github"></i>
            </a>
            <a href="https://orcid.org/0009-0006-6580-3711" title="ORCID" target="_blank">
                <i class="fa-brands fa-orcid"></i>
            </a>
            <a href="files/CV.pdf" title="CV" target="_blank">
                <i class="fas fa-file"></i>
            </a>
        </div>
    </div>

    <div class="section">
        <h2>Publications</h2>
        <div class="publication">
            <div class="publication-title">Your Research Paper Title</div>
            <div class="publication-authors">Erdem Önal, Co-author Name</div>
            <div class="publication-venue">Conference/Journal Name, 2024</div>
        </div>
        <div class="publication">
            <div class="publication-title">Another Research Contribution</div>
            <div class="publication-authors">Erdem Önal</div>
            <div class="publication-venue">Academic Venue, 2023</div>
        </div>
    </div>

    <div class="section">
        <h2>Research</h2>
        <div class="research">
            <div class="research-title">A Systematic Mapping Study on Green Load<br>Balancing Algorithms for Cloud Data Centers</div>
            <div class="research-authors">Erdem Önal, Fadi Alkhori, Marvin Schramm</div>
            <a href="files/SMS-Study.pdf" class="research-paper-link" target="_blank">Paper</a>
        </div>
    </div>

    <div class="section">
        <h2>Notes</h2>
        <div class="note">
            <div class="note-title">Gradient Descent</div>
            <div class="note-description">Gradient descent finds local minima by iteratively moving in the direction of steepest descent using the formula xₙ₊₁ = xₙ - α∇f(xₙ), where α is the learning rate. The negative gradient -∇f(x) points towards the greatest decrease in function value. Learning rate selection balances convergence speed and stability because too small a rate converges slowly, while too large a learning rate may cause overshooting or divergence.</div>
        </div>
        <div class="note">
            <div class="note-title">Hessian Matrix</div>
            <div class="note-description">The Hessian matrix H describes the local curvature of scalar functions f(x₁, x₂, ..., xₙ). It is an n×n matrix where the entry (i,j) is ∂²f/∂xᵢ∂xⱼ. The matrix is symmetric when the second derivatives are continuous. In optimization, the Hessian at critical points indicates whether the function has a local minimum (positive definite), a local maximum (negative definite), or a saddle point (indefinite).</div>
        </div>
        <div class="note">
            <div class="note-title">Jacobian Matrix</div>
            <div class="note-description">The Jacobian matrix J is an m×n matrix of all partial derivatives of first degree of a vector function f: ℝⁿ → ℝᵐ. Its (i,j) element is ∂fᵢ/∂xⱼ. The Jacobian represents the best linear approximation of the function near a given point. For square matrices, the Jacobian determinant enables a change of variables in multiple integrals. When the determinant at a point is not zero, the Inverse Function Theorem states that the function is locally invertible with a continuously differentiable inverse.</div>
        </div>
        <div class="note">
            <div class="note-title">K-Nearest Neighbors</div>
            <div class="note-description">K-Nearest Neighbors is a lazy learning algorithm that predicts based on the K nearest neighbors to a query point. It uses a majority vote for classification and averages neighbor values for regression. The algorithm calculates distances between the query and training points, then selects the K closest. Choosing K requires a compromise because a small K is sensitive to noise, while a large K produces more gradual boundaries but may underfit. Odd K values are preferred in binary classification to avoid ties.</div>
        </div>
        <div class="note">
            <div class="note-title">Lagrange Multipliers</div>
            <div class="note-description">Lagrange multipliers solve constrained optimization problems of minimizing f(x) subject to g(x)=0 by forming the Lagrangian L(x,λ)=f(x)−λg(x). The solution requires setting all partial derivatives to zero, giving ∇f(x)=λ∇g(x) and g(x)=0. The multiplier λ indicates how the optimal value changes with respect to the constraint.</div>
        </div>
        <div class="note">
            <div class="note-title">Newton-Raphson Method</div>
            <div class="note-description">The Newton-Raphson method finds roots of the equation f(x)=0 using the iterative formula xₙ₊₁ = xₙ - f(xₙ)/f'(xₙ). Starting from an initial guess x₀, each iteration uses the tangent line at the current point to approximate where the function crosses the x-axis. The method converges when the initial guess is sufficiently close to the root, f'(x)≠0 near the root, and the function is continuously differentiable. Common convergence criteria are |xₙ₊₁ - xₙ| < ε or |f(xₙ)| < ε, where ε is a small tolerance.</div>
        </div>
    </div>

</body>
</html> 