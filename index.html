<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Erdem Önal</title>
    <link rel="icon" type="image/png" href="images/icon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="header">
        <h1>Erdem Önal</h1>
        <div class="intro">
            I am a first year MSc student in <a href="https://www.mines-stetienne.fr/formation/cyber-physical-social-systems-cps2/" class="program-link" target="_blank">Computer Science (CPS2: AI and IoT)</a> at Jean Monnet University and École des Mines de Saint-Étienne.<br><br>
            I am interested in how <span class="highlight-ml">machine learning</span> can be integrated into <span class="highlight-is">intelligent systems</span>.
        </div>
        <div class="social-links">
            <a href="mailto:erdemonal11@outlook.com" title="Email">
                <i class="fas fa-envelope"></i>
            </a>
            <a href="https://linkedin.com/in/erdemonal1" title="LinkedIn" target="_blank">
                <i class="fab fa-linkedin"></i>
            </a>
            <a href="https://github.com/erdemonal11" title="GitHub" target="_blank">
                <i class="fab fa-github"></i>
            </a>
            <a href="https://orcid.org/0009-0006-6580-3711" title="ORCID" target="_blank">
                <i class="fa-brands fa-orcid"></i>
            </a>
            <a href="files/CV.pdf" title="CV" target="_blank">
                <img src="images/cv.png" alt="CV" class="cv-icon" />
            </a>
        </div>
    </div>

    <div class="section">
        <h2>Publications</h2>
        <div class="publication">
            <div class="publication-title">Your Research Paper Title</div>
            <div class="publication-authors">Erdem Önal, Co-author Name</div>
            <div class="publication-venue">Conference/Journal Name, 2024</div>
        </div>
        <div class="publication">
            <div class="publication-title">Another Research Contribution</div>
            <div class="publication-authors">Erdem Önal</div>
            <div class="publication-venue">Academic Venue, 2023</div>
        </div>
    </div>

    <div class="section">
        <h2>Research</h2>
        <div class="research">
            <div class="research-title">A Systematic Mapping Study on Green Load<br>Balancing Algorithms for Cloud Data Centers</div>
            <div class="research-authors">Erdem  Önal, Fadi Alkhori, Marvin Schramm</div>
            <a href="files/SMS-Study.pdf" class="research-paper-link" target="_blank">Paper</a>
        </div>
    </div>

    <div class="section">
        <h2>Notes</h2>
        <div class="note">
            <div class="note-title">Hessian Matrix</div>
            <div class="note-description">The Hessian matrix H describes local curvature of scalar functions f(x₁, x₂, ..., xₙ). It is an n×n matrix where entry (i,j) is ∂²f/∂xᵢ∂xⱼ. The matrix is symmetric when second derivatives are continuous. In optimization, the Hessian at critical points indicates: positive definite means local minimum, negative definite means local maximum, and indefinite means saddle point.</div>
        </div>
        <div class="note">
            <div class="note-title">Jacobian Matrix</div>
            <div class="note-description">The Jacobian matrix J is an m×n matrix of all first-order partial derivatives of a vector-valued function f: ℝⁿ → ℝᵐ. Entry (i,j) is ∂fᵢ/∂xⱼ.<br>The Jacobian represents the best linear approximation of the function near a given point. For square matrices, the Jacobian determinant enables change of variables in multiple integrals. When the determinant is non-zero at a point, the Inverse Function Theorem guarantees the function is locally invertible with a continuously differentiable inverse.</div>
        </div>
        <div class="note">
            <div class="note-title">Newton-Raphson Method</div>
            <div class="note-description">The Newton-Raphson method finds roots of f(x)=0 using the iterative formula xₙ₊₁ = xₙ - f(xₙ)/f'(xₙ). Starting from initial guess x₀, each iteration uses the tangent line at the current point to approximate where the function crosses the x-axis.<br>The method converges when the initial guess is sufficiently close to the actual root, f'(x)≠0 near the root, and the function is smooth. Convergence criteria include |xₙ₊₁ - xₙ| &lt; ε or |f(xₙ)| &lt; ε, where ε is a small tolerance value.</div>
        </div>
        <div class="note">
            <div class="note-title">K-Nearest Neighbors (KNN)</div>
            <div class="note-description">K-Nearest Neighbors (KNN) is a lazy learning algorithm that makes predictions based on the K nearest neighbors to a query point. For classification, it uses majority vote among K neighbors; for regression, it averages K neighbor values.<br>The algorithm calculates distances (commonly Euclidean, Manhattan, or Minkowski) between the query point and all training points, then selects the K nearest neighbors. Choosing K involves a trade-off: small K is sensitive to noise, large K creates smoother boundaries but may underfit. Odd K values are preferred for binary classification to avoid ties.</div>
        </div>
        <div class="note">
            <div class="note-title">Lagrange Multipliers</div>
            <div class="note-description">Lagrange multipliers solve constrained optimization problems min f(x) subject to g(x)=0 by forming the Lagrangian L(x,λ) = f(x) - λg(x). The solution requires setting all partial derivatives to zero, yielding ∇f(x) = λ∇g(x) and g(x) = 0.<br>Geometrically, at the optimal point, the gradient of the objective function is parallel to the gradient of the constraint, meaning contour lines of f(x) are tangent to the constraint curve. The multiplier λ represents the rate of change of the optimal value with respect to the constraint. For multiple constraints, the Lagrangian becomes L(x,λ₁,...,λₘ) = f(x) - Σλᵢgᵢ(x).</div>
        </div>
    </div>


</body>
</html> 